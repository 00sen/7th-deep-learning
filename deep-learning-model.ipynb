{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Model\n",
    "## Luis Arturo\n",
    "### A01703572"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conexión a GPU local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import setuptools\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n",
      "\n",
      "CUDA available: True\n",
      "\n",
      "Available devices:\n",
      "  CPU: /physical_device:CPU:0\n",
      "\n",
      "✅ Successfully performed computation on GPU!\n",
      "Matrix multiplication shape: (1000, 1000)\n",
      "\n",
      "❌ No GPU devices found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1730428731.716877    6613 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "def verify_gpu_setup():\n",
    "    \"\"\"Comprehensive verification of TensorFlow GPU setup\"\"\"\n",
    "    # Print TensorFlow version\n",
    "    print(f\"TensorFlow version: {tf.__version__}\")\n",
    "    \n",
    "    # Check CUDA availability\n",
    "    print(\"\\nCUDA available:\", tf.test.is_built_with_cuda())\n",
    "    \n",
    "    # List all available GPUs\n",
    "    physical_devices = tf.config.list_physical_devices()\n",
    "    print(\"\\nAvailable devices:\")\n",
    "    for device in physical_devices:\n",
    "        print(f\"  {device.device_type}: {device.name}\")\n",
    "    \n",
    "    # Check if GPU is available and perform a simple computation\n",
    "    if tf.test.is_built_with_cuda():\n",
    "        try:\n",
    "            # Create some random data\n",
    "            with tf.device('/GPU:0'):\n",
    "                a = tf.random.normal([1000, 1000])\n",
    "                b = tf.random.normal([1000, 1000])\n",
    "                # Perform matrix multiplication\n",
    "                c = tf.matmul(a, b)\n",
    "                # Force execution\n",
    "                result = c.numpy()\n",
    "                print(\"\\n✅ Successfully performed computation on GPU!\")\n",
    "                print(f\"Matrix multiplication shape: {result.shape}\")\n",
    "        except Exception as e:\n",
    "            print(\"\\n❌ Error during GPU computation:\")\n",
    "            print(e)\n",
    "    else:\n",
    "        print(\"\\n❌ CUDA is not available in this TensorFlow installation\")\n",
    "\n",
    "    # Print GPU memory info\n",
    "    try:\n",
    "        gpu = tf.config.list_physical_devices('GPU')[0]\n",
    "        print(\"\\nGPU Device:\", gpu)\n",
    "    except IndexError:\n",
    "        print(\"\\n❌ No GPU devices found\")\n",
    "\n",
    "verify_gpu_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set TF logging level to avoid unnecessary messages\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración para mostrar todas las columnas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CARGANDO DATASETS ===\n",
      "=== DATASETS CARGADOS ===\n"
     ]
    }
   ],
   "source": [
    "# 1. EXTRACT - Carga de datos\n",
    "print(\"=== CARGANDO DATASETS ===\")\n",
    "games_df = pd.read_csv('data/games.csv')\n",
    "users_df = pd.read_csv('data/users.csv')\n",
    "recommendations_df = pd.read_csv('data/recommendations.csv')\n",
    "\n",
    "print(\"=== DATASETS CARGADOS ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Información general del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensiones del games_df: (50872, 13)\n",
      "\n",
      "Columnas disponibles del games_df:\n",
      "- app_id\n",
      "- title\n",
      "- date_release\n",
      "- win\n",
      "- mac\n",
      "- linux\n",
      "- rating\n",
      "- positive_ratio\n",
      "- user_reviews\n",
      "- price_final\n",
      "- price_original\n",
      "- discount\n",
      "- steam_deck\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDimensiones del games_df:\", games_df.shape)\n",
    "print(\"\\nColumnas disponibles del games_df:\")\n",
    "for col in games_df.columns:\n",
    "    print(f\"- {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensiones del users_df: (14306064, 3)\n",
      "\n",
      "Columnas disponibles del users_df:\n",
      "- user_id\n",
      "- products\n",
      "- reviews\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDimensiones del users_df:\", users_df.shape)\n",
    "print(\"\\nColumnas disponibles del users_df:\")\n",
    "for col in users_df.columns:\n",
    "    print(f\"- {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensiones del recommendations_df: (41154794, 8)\n",
      "\n",
      "Columnas disponibles del recommendations_df:\n",
      "- app_id\n",
      "- helpful\n",
      "- funny\n",
      "- date\n",
      "- is_recommended\n",
      "- hours\n",
      "- user_id\n",
      "- review_id\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDimensiones del recommendations_df:\", recommendations_df.shape)\n",
    "print(\"\\nColumnas disponibles del recommendations_df:\")\n",
    "for col in recommendations_df.columns:\n",
    "    print(f\"- {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis inicial de games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Primeras 5 filas de games_df:\n",
      "   app_id                              title date_release   win    mac  linux  \\\n",
      "0   13500  Prince of Persia: Warrior Within™   2008-11-21  True  False  False   \n",
      "1   22364            BRINK: Agents of Change   2011-08-03  True  False  False   \n",
      "2  113020       Monaco: What's Yours Is Mine   2013-04-24  True   True   True   \n",
      "3  226560                 Escape Dead Island   2014-11-18  True  False  False   \n",
      "4  249050            Dungeon of the ENDLESS™   2014-10-27  True   True  False   \n",
      "\n",
      "          rating  positive_ratio  user_reviews  price_final  price_original  \\\n",
      "0  Very Positive              84          2199         9.99            9.99   \n",
      "1       Positive              85            21         2.99            2.99   \n",
      "2  Very Positive              92          3722        14.99           14.99   \n",
      "3          Mixed              61           873        14.99           14.99   \n",
      "4  Very Positive              88          8784        11.99           11.99   \n",
      "\n",
      "   discount  steam_deck  \n",
      "0       0.0        True  \n",
      "1       0.0        True  \n",
      "2       0.0        True  \n",
      "3       0.0        True  \n",
      "4       0.0        True  \n",
      "\n",
      "Información del dataset de juegos:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50872 entries, 0 to 50871\n",
      "Data columns (total 13 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   app_id          50872 non-null  int64  \n",
      " 1   title           50872 non-null  object \n",
      " 2   date_release    50872 non-null  object \n",
      " 3   win             50872 non-null  bool   \n",
      " 4   mac             50872 non-null  bool   \n",
      " 5   linux           50872 non-null  bool   \n",
      " 6   rating          50872 non-null  object \n",
      " 7   positive_ratio  50872 non-null  int64  \n",
      " 8   user_reviews    50872 non-null  int64  \n",
      " 9   price_final     50872 non-null  float64\n",
      " 10  price_original  50872 non-null  float64\n",
      " 11  discount        50872 non-null  float64\n",
      " 12  steam_deck      50872 non-null  bool   \n",
      "dtypes: bool(4), float64(3), int64(3), object(3)\n",
      "memory usage: 3.7+ MB\n",
      "None\n",
      "\n",
      "Estadísticas descriptivas de games_df:\n",
      "             app_id  positive_ratio  user_reviews   price_final  \\\n",
      "count  5.087200e+04    50872.000000  5.087200e+04  50872.000000   \n",
      "mean   1.055224e+06       77.052033  1.824425e+03      8.620325   \n",
      "std    6.103249e+05       18.253592  4.007352e+04     11.514164   \n",
      "min    1.000000e+01        0.000000  1.000000e+01      0.000000   \n",
      "25%    5.287375e+05       67.000000  1.900000e+01      0.990000   \n",
      "50%    9.860850e+05       81.000000  4.900000e+01      4.990000   \n",
      "75%    1.524895e+06       91.000000  2.060000e+02     10.990000   \n",
      "max    2.599300e+06      100.000000  7.494460e+06    299.990000   \n",
      "\n",
      "       price_original      discount  \n",
      "count    50872.000000  50872.000000  \n",
      "mean         8.726788      5.592212  \n",
      "std         11.507021     18.606679  \n",
      "min          0.000000      0.000000  \n",
      "25%          0.990000      0.000000  \n",
      "50%          4.990000      0.000000  \n",
      "75%         11.990000      0.000000  \n",
      "max        299.990000     90.000000  \n",
      "\n",
      "Valores nulos en games_df:\n",
      "app_id            0\n",
      "title             0\n",
      "date_release      0\n",
      "win               0\n",
      "mac               0\n",
      "linux             0\n",
      "rating            0\n",
      "positive_ratio    0\n",
      "user_reviews      0\n",
      "price_final       0\n",
      "price_original    0\n",
      "discount          0\n",
      "steam_deck        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPrimeras 5 filas de games_df:\")\n",
    "print(games_df.head())\n",
    "print(\"\\nInformación del dataset de juegos:\")\n",
    "print(games_df.info())\n",
    "print(\"\\nEstadísticas descriptivas de games_df:\")\n",
    "print(games_df.describe())\n",
    "print(\"\\nValores nulos en games_df:\")\n",
    "print(games_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis inicial de users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Primeras 5 filas de users_df:\n",
      "    user_id  products  reviews\n",
      "0   7360263       359        0\n",
      "1  14020781       156        1\n",
      "2   8762579       329        4\n",
      "3   4820647       176        4\n",
      "4   5167327        98        2\n",
      "\n",
      "Información del dataset de usuarios:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14306064 entries, 0 to 14306063\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Dtype\n",
      "---  ------    -----\n",
      " 0   user_id   int64\n",
      " 1   products  int64\n",
      " 2   reviews   int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 327.4 MB\n",
      "None\n",
      "\n",
      "Estadísticas descriptivas de users_df:\n",
      "            user_id      products       reviews\n",
      "count  1.430606e+07  1.430606e+07  1.430606e+07\n",
      "mean   7.153032e+06  1.163734e+02  2.876738e+00\n",
      "std    4.129805e+06  2.438515e+02  7.987421e+00\n",
      "min    0.000000e+00  0.000000e+00  0.000000e+00\n",
      "25%    3.576516e+06  2.300000e+01  1.000000e+00\n",
      "50%    7.153032e+06  5.500000e+01  1.000000e+00\n",
      "75%    1.072955e+07  1.270000e+02  3.000000e+00\n",
      "max    1.430606e+07  3.221400e+04  6.045000e+03\n",
      "\n",
      "Valores nulos en users_df:\n",
      "user_id     0\n",
      "products    0\n",
      "reviews     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPrimeras 5 filas de users_df:\")\n",
    "print(users_df.head())\n",
    "print(\"\\nInformación del dataset de usuarios:\")\n",
    "print(users_df.info())\n",
    "print(\"\\nEstadísticas descriptivas de users_df:\")\n",
    "print(users_df.describe())\n",
    "print(\"\\nValores nulos en users_df:\")\n",
    "print(users_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis inicial de recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Primeras 5 filas de recommendations_df:\n",
      "    app_id  helpful  funny        date  is_recommended  hours  user_id  \\\n",
      "0   975370        0      0  2022-12-12            True   36.3    51580   \n",
      "1   304390        4      0  2017-02-17           False   11.5     2586   \n",
      "2  1085660        2      0  2019-11-17            True  336.5   253880   \n",
      "3   703080        0      0  2022-09-23            True   27.4   259432   \n",
      "4   526870        0      0  2021-01-10            True    7.9    23869   \n",
      "\n",
      "   review_id  \n",
      "0          0  \n",
      "1          1  \n",
      "2          2  \n",
      "3          3  \n",
      "4          4  \n",
      "\n",
      "Información del dataset de recomendaciones:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41154794 entries, 0 to 41154793\n",
      "Data columns (total 8 columns):\n",
      " #   Column          Dtype  \n",
      "---  ------          -----  \n",
      " 0   app_id          int64  \n",
      " 1   helpful         int64  \n",
      " 2   funny           int64  \n",
      " 3   date            object \n",
      " 4   is_recommended  bool   \n",
      " 5   hours           float64\n",
      " 6   user_id         int64  \n",
      " 7   review_id       int64  \n",
      "dtypes: bool(1), float64(1), int64(5), object(1)\n",
      "memory usage: 2.2+ GB\n",
      "None\n",
      "\n",
      "Estadísticas descriptivas de recommendations_df:\n",
      "             app_id       helpful         funny         hours       user_id  \\\n",
      "count  4.115479e+07  4.115479e+07  4.115479e+07  4.115479e+07  4.115479e+07   \n",
      "mean   6.032724e+05  3.202567e+00  1.058071e+00  1.006022e+02  7.450576e+06   \n",
      "std    4.729233e+05  4.693649e+01  2.867060e+01  1.761675e+02  4.010685e+06   \n",
      "min    1.000000e+01  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%    2.539400e+05  0.000000e+00  0.000000e+00  7.800000e+00  4.287256e+06   \n",
      "50%    4.351500e+05  0.000000e+00  0.000000e+00  2.730000e+01  7.546446e+06   \n",
      "75%    9.331100e+05  0.000000e+00  0.000000e+00  9.920000e+01  1.096877e+07   \n",
      "max    2.253290e+06  3.621200e+04  2.810900e+04  1.000000e+03  1.430606e+07   \n",
      "\n",
      "          review_id  \n",
      "count  4.115479e+07  \n",
      "mean   2.057740e+07  \n",
      "std    1.188037e+07  \n",
      "min    0.000000e+00  \n",
      "25%    1.028870e+07  \n",
      "50%    2.057740e+07  \n",
      "75%    3.086609e+07  \n",
      "max    4.115479e+07  \n",
      "\n",
      "Valores nulos en recommendations_df:\n",
      "app_id            0\n",
      "helpful           0\n",
      "funny             0\n",
      "date              0\n",
      "is_recommended    0\n",
      "hours             0\n",
      "user_id           0\n",
      "review_id         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPrimeras 5 filas de recommendations_df:\")\n",
    "print(recommendations_df.head())\n",
    "print(\"\\nInformación del dataset de recomendaciones:\")\n",
    "print(recommendations_df.info())\n",
    "print(\"\\nEstadísticas descriptivas de recommendations_df:\")\n",
    "print(recommendations_df.describe())\n",
    "print(\"\\nValores nulos en recommendations_df:\")\n",
    "print(recommendations_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis expecífico de features importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribución de ratings en juegos:\n",
      "rating\n",
      "Positive                   13502\n",
      "Very Positive              13139\n",
      "Mixed                      12157\n",
      "Mostly Positive             8738\n",
      "Mostly Negative             1849\n",
      "Overwhelmingly Positive     1110\n",
      "Negative                     303\n",
      "Very Negative                 60\n",
      "Overwhelmingly Negative       14\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Análisis de ratings\n",
    "print(\"\\nDistribución de ratings en juegos:\")\n",
    "print(games_df['rating'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estadísticas de precios:\n",
      "        price_final  price_original\n",
      "count  50872.000000    50872.000000\n",
      "mean       8.620325        8.726788\n",
      "std       11.514164       11.507021\n",
      "min        0.000000        0.000000\n",
      "25%        0.990000        0.990000\n",
      "50%        4.990000        4.990000\n",
      "75%       10.990000       11.990000\n",
      "max      299.990000      299.990000\n"
     ]
    }
   ],
   "source": [
    "# Análisis de precios\n",
    "print(\"\\nEstadísticas de precios:\")\n",
    "print(games_df[['price_final', 'price_original']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribución de recomendaciones:\n",
      "is_recommended\n",
      "True     0.857844\n",
      "False    0.142156\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Análisis de recomendaciones\n",
    "print(\"\\nDistribución de recomendaciones:\")\n",
    "print(recommendations_df['is_recommended'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estadísticas de horas jugadas:\n",
      "count    4.115479e+07\n",
      "mean     1.006022e+02\n",
      "std      1.761675e+02\n",
      "min      0.000000e+00\n",
      "25%      7.800000e+00\n",
      "50%      2.730000e+01\n",
      "75%      9.920000e+01\n",
      "max      1.000000e+03\n",
      "Name: hours, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Análisis de horas jugadas\n",
    "print(\"\\nEstadísticas de horas jugadas:\")\n",
    "print(recommendations_df['hours'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificamos la integridad de referencias entre los datasets\n",
    "Nos aseguramos de que los juegos y los usuarios referenciados en reseñas existen en el dataset de juegos y en el de usuarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Juegos en recommendations pero no en games: 0\n"
     ]
    }
   ],
   "source": [
    "# Verificar si todos los app_id en recommendations existen en games\n",
    "games_apps = set(games_df['app_id'])\n",
    "recommendations_apps = set(recommendations_df['app_id'])\n",
    "print(\"\\nJuegos en recommendations pero no en games:\", len(recommendations_apps - games_apps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usuarios en recommendations pero no en users: 0\n"
     ]
    }
   ],
   "source": [
    "# Verificar si todos los user_id en recommendations existen en users\n",
    "users_ids = set(users_df['user_id'])\n",
    "recommendations_users = set(recommendations_df['user_id'])\n",
    "print(\"Usuarios en recommendations pero no en users:\", len(recommendations_users - users_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Se va a crear un modelo capaz de predecir el rating que tendrá un juego (\"Very Positive\", \"Mixed\", etc.) \n",
    "## Son 9 clases \n",
    "- Positive                   13502\n",
    "- Very Positive              13139\n",
    "- Mixed                      12157\n",
    "- Mostly Positive             8738\n",
    "- Mostly Negative             1849\n",
    "- Overwhelmingly Positive     1110\n",
    "- Negative                     303\n",
    "- Very Negative                 60\n",
    "- Overwhelmingly Negative       14\n",
    "\n",
    "## Features de entrada\n",
    "- Precio\n",
    "- Plataformas soportadas (win, mac, linux)\n",
    "- Tiempo en el mercado\n",
    "- Descuentos\n",
    "- Métricas agregadas de reviews (horas promedio jugadas, ratio de recomendaciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiendo de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función para juntar el dataset de recommendations y el de games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_games_data(games_df, recommendations_df):\n",
    "    \"\"\"\n",
    "    Enriquece el dataset de juegos con métricas agregadas de recomendaciones\n",
    "    \"\"\"\n",
    "    # Calculamos métricas agregadas por juego\n",
    "    agg_metrics = recommendations_df.groupby('app_id').agg({\n",
    "        'hours': ['mean', 'median', 'std'],\n",
    "        'is_recommended': 'mean',\n",
    "        'helpful': 'mean',\n",
    "        'funny': 'mean'\n",
    "    }).round(3)\n",
    "    \n",
    "    # Aplanamos los nombres de las columnas\n",
    "    agg_metrics.columns = [\n",
    "        'avg_hours', 'median_hours', 'std_hours',\n",
    "        'recommendation_ratio', 'avg_helpful', 'avg_funny'\n",
    "    ]\n",
    "    \n",
    "    # Mergeamos con el dataset original de juegos\n",
    "    enriched_games = games_df.merge(\n",
    "        agg_metrics, \n",
    "        left_on='app_id', \n",
    "        right_index=True, \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Llenamos valores NaN (juegos sin recomendaciones)\n",
    "    enriched_games = enriched_games.fillna({\n",
    "        'avg_hours': 0,\n",
    "        'median_hours': 0,\n",
    "        'std_hours': 0,\n",
    "        'recommendation_ratio': 0,\n",
    "        'avg_helpful': 0,\n",
    "        'avg_funny': 0\n",
    "    })\n",
    "    \n",
    "    return enriched_games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creamos al preprocesador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameRatingPreprocessor:\n",
    "    def __init__(self, max_title_length=10):\n",
    "        # Inicializamos encoders y tokenizers\n",
    "        self.title_tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "        self.developer_encoder = LabelEncoder()\n",
    "        self.publisher_encoder = LabelEncoder()\n",
    "        self.rating_encoder = LabelEncoder()\n",
    "        self.scalers = {}\n",
    "        \n",
    "        # Parámetros de configuración\n",
    "        self.max_title_length = max_title_length\n",
    "        self.vocab_size = None\n",
    "        self.num_developers = None\n",
    "        self.num_publishers = None\n",
    "        self.num_ratings = None\n",
    "        \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Limpia y normaliza el texto del título\"\"\"\n",
    "        # Convertimos el texto a minúsculas\n",
    "        text = text.lower()\n",
    "        # Quitamos cualquier carater especial pero dejamos los espacios\n",
    "        text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def process_numerical_features(self, df):\n",
    "        \"\"\"Procesa y escala características numéricas\"\"\"\n",
    "        numerical_features = [\n",
    "            'price_final', \n",
    "            'price_original', \n",
    "            'discount',\n",
    "            'positive_ratio',\n",
    "            'user_reviews'\n",
    "        ]\n",
    "        \n",
    "        # Crear features adicionales con manejo de infinitos\n",
    "        # Reemplazar infinitos o valores muy grandes con NaN y luego con 1.0\n",
    "        df_copy['price_ratio'] = (df_copy['price_final'] / df_copy['price_original']).replace([np.inf, -np.inf], np.nan).fillna(1.0)\n",
    "        df_copy['platform_count'] = df_copy['win'].astype(int) + df_copy['mac'].astype(int) + df_copy['linux'].astype(int)\n",
    "        \n",
    "        # Agregar features derivados\n",
    "        numerical_features.extend(['price_ratio', 'platform_count'])\n",
    "        \n",
    "        # Escalar features numéricos\n",
    "        X_numerical = df[numerical_features].copy()\n",
    "        for column in numerical_features:\n",
    "            scaler = StandardScaler()\n",
    "            X_numerical[column] = scaler.fit_transform(X_numerical[[column]])\n",
    "            self.scalers[column] = scaler\n",
    "            \n",
    "        return X_numerical\n",
    "    \n",
    "    def process_categorical_features(self, df):\n",
    "        \"\"\"Procesa características categóricas y crea embeddings\"\"\"\n",
    "        # Procesar títulos\n",
    "        cleaned_titles = df['title'].apply(self.clean_text)\n",
    "        self.title_tokenizer.fit_on_texts(cleaned_titles)\n",
    "        self.vocab_size = len(self.title_tokenizer.word_index) + 1\n",
    "        title_sequences = self.title_tokenizer.texts_to_sequences(cleaned_titles)\n",
    "        X_titles = pad_sequences(title_sequences, maxlen=self.max_title_length)\n",
    "        \n",
    "        # Procesar desarrolladores\n",
    "        developers = df['developer'].apply(eval)\n",
    "        primary_developers = developers.apply(lambda x: x[0] if x else 'Unknown')\n",
    "        X_developers = self.developer_encoder.fit_transform(primary_developers)\n",
    "        self.num_developers = len(self.developer_encoder.classes_)\n",
    "        \n",
    "        # Procesar publishers\n",
    "        publishers = df['publisher'].apply(eval)\n",
    "        primary_publishers = publishers.apply(lambda x: x[0] if x else 'Unknown')\n",
    "        X_publishers = self.publisher_encoder.fit_transform(primary_publishers)\n",
    "        self.num_publishers = len(self.publisher_encoder.classes_)\n",
    "        \n",
    "        return X_titles, X_developers, X_publishers\n",
    "    \n",
    "    def process_target(self, ratings):\n",
    "        \"\"\"Procesa la variable objetivo (ratings)\"\"\"\n",
    "        # Crear mapping ordenado de ratings\n",
    "        rating_order = [\n",
    "            'Overwhelmingly Negative',\n",
    "            'Very Negative',\n",
    "            'Negative',\n",
    "            'Mostly Negative',\n",
    "            'Mixed',\n",
    "            'Mostly Positive',\n",
    "            'Positive',\n",
    "            'Very Positive',\n",
    "            'Overwhelmingly Positive'\n",
    "        ]\n",
    "        \n",
    "        # Encodificar ratings manteniendo el orden\n",
    "        self.rating_encoder.fit(rating_order)\n",
    "        y = self.rating_encoder.transform(ratings)\n",
    "        self.num_ratings = len(rating_order)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def prepare_data(self, games_df, test_size=0.2, val_size=0.2):\n",
    "        \"\"\"Prepara todos los datos para el entrenamiento\"\"\"\n",
    "        # Procesar features\n",
    "        X_numerical = self.process_numerical_features(games_df)\n",
    "        print(\"ola k ase\")\n",
    "        X_titles, X_developers, X_publishers = self.process_categorical_features(games_df)\n",
    "        y = self.process_target(games_df['rating'])\n",
    "        \n",
    "        # Dividir los datos\n",
    "        indices = np.arange(len(games_df))\n",
    "        indices_train_val, indices_test = train_test_split(\n",
    "            indices, test_size=test_size, random_state=42, stratify=y\n",
    "        )\n",
    "        indices_train, indices_val = train_test_split(\n",
    "            indices_train_val, test_size=val_size, random_state=42, \n",
    "            stratify=y[indices_train_val]\n",
    "        )\n",
    "        \n",
    "        # Función helper para dividir datos\n",
    "        def split_data(X, indices_train, indices_val, indices_test):\n",
    "            return (\n",
    "                X[indices_train], \n",
    "                X[indices_val], \n",
    "                X[indices_test]\n",
    "            )\n",
    "        \n",
    "        # Preparar todos los conjuntos de datos\n",
    "        return {\n",
    "            'numerical': split_data(X_numerical.values, indices_train, indices_val, indices_test),\n",
    "            'titles': split_data(X_titles, indices_train, indices_val, indices_test),\n",
    "            'developers': split_data(X_developers, indices_train, indices_val, indices_test),\n",
    "            'publishers': split_data(X_publishers, indices_train, indices_val, indices_test),\n",
    "            'target': split_data(y, indices_train, indices_val, indices_test)\n",
    "        }\n",
    "    \n",
    "    def get_feature_dims(self):\n",
    "        \"\"\"Retorna las dimensiones de las features para el modelo\"\"\"\n",
    "        return {\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'num_developers': self.num_developers,\n",
    "            'num_publishers': self.num_publishers,\n",
    "            'num_numerical': len(self.scalers),\n",
    "            'num_ratings': self.num_ratings\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamos el nuevo dataset enriquecido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m GameRatingPreprocessor()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Preparamos los datos\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m prepared_data \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43menriched_games\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 99\u001b[0m, in \u001b[0;36mGameRatingPreprocessor.prepare_data\u001b[0;34m(self, games_df, test_size, val_size)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prepara todos los datos para el entrenamiento\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Procesar features\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m X_numerical \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_numerical_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgames_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mola k ase\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    101\u001b[0m X_titles, X_developers, X_publishers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_categorical_features(games_df)\n",
      "Cell \u001b[0;32mIn[30], line 46\u001b[0m, in \u001b[0;36mGameRatingPreprocessor.process_numerical_features\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m numerical_features:\n\u001b[1;32m     45\u001b[0m     scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m---> 46\u001b[0m     X_numerical[column] \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_numerical\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscalers[column] \u001b[38;5;241m=\u001b[39m scaler\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_numerical\n",
      "File \u001b[0;32m~/zProgramming/7th-deep-learning/.venv/lib/python3.12/site-packages/sklearn/utils/_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    322\u001b[0m         )\n",
      "File \u001b[0;32m~/zProgramming/7th-deep-learning/.venv/lib/python3.12/site-packages/sklearn/base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1084\u001b[0m             (\n\u001b[1;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m   1094\u001b[0m         )\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/zProgramming/7th-deep-learning/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:878\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 878\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/zProgramming/7th-deep-learning/.venv/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/zProgramming/7th-deep-learning/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:914\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    882\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[1;32m    883\u001b[0m \n\u001b[1;32m    884\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    913\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 914\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    921\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/zProgramming/7th-deep-learning/.venv/lib/python3.12/site-packages/sklearn/base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/zProgramming/7th-deep-learning/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1061\u001b[0m     )\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m-> 1064\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1073\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m~/zProgramming/7th-deep-learning/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/zProgramming/7th-deep-learning/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    171\u001b[0m     )\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# Enriquecemos el dataset de juegos\n",
    "enriched_games = enrich_games_data(games_df, recommendations_df)\n",
    "\n",
    "# Creamos el preprocesador\n",
    "preprocessor = GameRatingPreprocessor()\n",
    "\n",
    "# Preparamos los datos\n",
    "prepared_data = preprocessor.prepare_data(enriched_games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of         app_id                                      title date_release   win  \\\n",
       "0        13500          Prince of Persia: Warrior Within™   2008-11-21  True   \n",
       "1        22364                    BRINK: Agents of Change   2011-08-03  True   \n",
       "2       113020               Monaco: What's Yours Is Mine   2013-04-24  True   \n",
       "3       226560                         Escape Dead Island   2014-11-18  True   \n",
       "4       249050                    Dungeon of the ENDLESS™   2014-10-27  True   \n",
       "...        ...                                        ...          ...   ...   \n",
       "50867  2296380  I Expect You To Die 3: Cog in the Machine   2023-09-28  True   \n",
       "50868  1272080                                   PAYDAY 3   2023-09-21  True   \n",
       "50869  1402110                                 Eternights   2023-09-11  True   \n",
       "50870  2272250                        Forgive Me Father 2   2023-10-19  True   \n",
       "50871  2488510                                  FatalZone   2023-10-23  True   \n",
       "\n",
       "         mac  linux           rating  positive_ratio  user_reviews  \\\n",
       "0      False  False    Very Positive              84          2199   \n",
       "1      False  False         Positive              85            21   \n",
       "2       True   True    Very Positive              92          3722   \n",
       "3      False  False            Mixed              61           873   \n",
       "4       True  False    Very Positive              88          8784   \n",
       "...      ...    ...              ...             ...           ...   \n",
       "50867  False  False    Very Positive              96           101   \n",
       "50868  False  False  Mostly Negative              38         29458   \n",
       "50869  False  False    Very Positive              89          1128   \n",
       "50870  False  False    Very Positive              95            82   \n",
       "50871  False  False    Very Positive              88           144   \n",
       "\n",
       "       price_final  price_original  discount  steam_deck  avg_hours  \\\n",
       "0             9.99            9.99       0.0        True     18.968   \n",
       "1             2.99            2.99       0.0        True      0.000   \n",
       "2            14.99           14.99       0.0        True     20.413   \n",
       "3            14.99           14.99       0.0        True     10.777   \n",
       "4            11.99           11.99       0.0        True     40.622   \n",
       "...            ...             ...       ...         ...        ...   \n",
       "50867        22.00            0.00       0.0        True      0.000   \n",
       "50868        40.00            0.00       0.0        True      0.000   \n",
       "50869        30.00            0.00       0.0        True      0.000   \n",
       "50870        17.00            0.00       0.0        True      0.000   \n",
       "50871         4.00            0.00       0.0        True      0.000   \n",
       "\n",
       "       median_hours  std_hours  recommendation_ratio  avg_helpful  avg_funny  \n",
       "0              12.9     41.722                 0.846        5.392      0.612  \n",
       "1               0.0      0.000                 0.000        0.000      0.000  \n",
       "2               6.9     53.295                 0.909        1.435      0.450  \n",
       "3               8.1     15.422                 0.626        6.779      0.815  \n",
       "4              23.4     54.860                 0.886        2.531      0.698  \n",
       "...             ...        ...                   ...          ...        ...  \n",
       "50867           0.0      0.000                 0.000        0.000      0.000  \n",
       "50868           0.0      0.000                 0.000        0.000      0.000  \n",
       "50869           0.0      0.000                 0.000        0.000      0.000  \n",
       "50870           0.0      0.000                 0.000        0.000      0.000  \n",
       "50871           0.0      0.000                 0.000        0.000      0.000  \n",
       "\n",
       "[50872 rows x 19 columns]>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enriched_games.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos las dimensiones y estructura de los datos\n",
    "print(\"\\nDimensiones de los datos preparados:\")\n",
    "for key, (train, val, test) in prepared_data.items():\n",
    "    print(f\"{key}:\")\n",
    "    print(f\"  Train: {train.shape}\")\n",
    "    print(f\"  Validation: {val.shape}\")\n",
    "    print(f\"  Test: {test.shape}\")\n",
    "\n",
    "print(\"\\nDimensiones de features para el modelo:\")\n",
    "for key, value in feature_dims.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
